3 - The Language of (non)Replicability Michal Herzenstein, University of Delaware, Newark, DE Sanjana Rosario, Shin Oblander, Oded Netzer We explore whether the language used by scholars in academic publications is predictive of whether the main findings in the paper are replicable. We build on past research showing that written words carry information beyond their literal meaning in interpersonal and formal communications, and even when the text is written and edited by multiple people. We join the emerging movement towards Open Science that aims to increase the openness, integrity, and reproducibility of scholarly research. So far,  replication attempts of hundreds of papers in psychology and economics demonstrate that less than half of the findings are replicable. We attempt to predict the replicability of nearly 300 Open Science pre-registered replications by focusing on the language used in the replicated papers. Controlling for a large set of metadata variables from the papers being replicated, including authors’ information, academic domain, and study-specific information such as sample and effects sizes, we find that the text used to describe the experiment being replicated improves predictions of replicability by over 4% (AUC with textual data = 0.734 versus, AUC without textual data = 0.705). This result holds in multiple robustness tests. Turning to interpretability, we find that replicable studies are often easier to read and include more quantitative and function words, while non-replicable studies are written with more clout and include more positive emotion words. Overall, we find that consciously or unconsciously, knowingly or unknowingly, researchers’ language usage alludes to their research replicability likelihood. Saturday, 9–10am 